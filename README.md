![image_grid](https://github.com/user-attachments/assets/cd9ba73d-c09c-4fb8-b2f4-6d32b49dff55)

> [!NOTE]
> You can find more information about my findings in the [blog post](https://kcm.sh) I wrote

# TODO
- Read TiTok paper
- Find implementation of it
- Add MoNe Decoder + MRL Tokens to TiTok
- Compare to MoVQGAN (read paper?)
- Fix up readme / drawio / blog
- Share w dude

Links for dataset:
- [Mondrian](https://pages.cs.wisc.edu/~andrzeje/research/mondrian.html)
- [Where's Waldo](https://www.kaggle.com/datasets/residentmario/wheres-waldo/)
- [Rocks](https://www.kaggle.com/datasets/neelgajare/rocks-dataset)

Sections of the ViT code were taken from Google's [Scenic](https://github.com/google-research/scenic) codebase

Open source implementation of SBER-MoVQGAN can be found [here](https://github.com/ai-forever/MoVQGAN). You can rerun the benchmarks using this notebook (ICON HERE)

## Results

CLIP and FID scores?
Perceptual Quality scores?